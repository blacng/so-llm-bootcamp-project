{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Agent Visualization\n",
    "\n",
    "This notebook visualizes the LangGraph agent structures used in the LLM Bootcamp Project.\n",
    "\n",
    "## Agent Architectures\n",
    "\n",
    "The project implements two main LangGraph-based agents:\n",
    "1. **Agentic RAG Workflow** - Intelligent document question-answering\n",
    "2. **ReAct Agent** - Web search-enabled conversational agent (via `create_react_agent`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install langchain-openai langgraph graphviz\n",
    "\n",
    "from typing import TypedDict, Literal, List\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Set OpenAI API key (optional - only needed if you want to run the graph)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agentic RAG Workflow\n",
    "\n",
    "The Agentic RAG workflow implements an intelligent document retrieval and generation system with three main nodes:\n",
    "\n",
    "### Node Structure:\n",
    "- **START** â†’ Entry point\n",
    "- **classify_mode** â†’ Determines if query requires \"summary\" or \"fact\" response\n",
    "- **retrieve** â†’ Fetches relevant documents (8 for summary, 3 for facts)\n",
    "- **generate** â†’ Creates grounded response from retrieved context\n",
    "- **END** â†’ Terminal node\n",
    "\n",
    "### State Management:\n",
    "The workflow uses a typed state dictionary (`RAGState`) to pass information between nodes:\n",
    "```python\n",
    "class RAGState(TypedDict):\n",
    "    question: str                      # User's query\n",
    "    mode: Literal[\"summary\", \"fact\"]  # Response strategy\n",
    "    documents: List[Document]          # Retrieved context\n",
    "    generation: str                    # Final response\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RAG state structure\n",
    "class RAGState(TypedDict):\n",
    "    \"\"\"State definition for the RAG workflow.\"\"\"\n",
    "\n",
    "    question: str\n",
    "    mode: Literal[\"summary\", \"fact\"]\n",
    "    documents: List[Document]\n",
    "    generation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node functions (simplified for visualization)\n",
    "def classify_mode(state: RAGState) -> RAGState:\n",
    "    \"\"\"Node 1: Classify query type to determine response strategy.\n",
    "\n",
    "    Analyzes the question to determine if user wants:\n",
    "    - Summary: Overview, key points, synthesis\n",
    "    - Fact: Specific details, dates, names, numbers\n",
    "    \"\"\"\n",
    "    # Summary hints: summarize, overview, key points, etc.\n",
    "    # Fact hints: when, who, where, amount, specific, etc.\n",
    "\n",
    "    # For demo, default to \"fact\" mode\n",
    "    return {**state, \"mode\": \"fact\"}\n",
    "\n",
    "\n",
    "def retrieve(state: RAGState) -> RAGState:\n",
    "    \"\"\"Node 2: Retrieve relevant documents based on query and mode.\n",
    "\n",
    "    Retrieval strategy:\n",
    "    - Summary mode: 8 documents (broader context)\n",
    "    - Fact mode: 3 documents (focused retrieval)\n",
    "    \"\"\"\n",
    "    # In real implementation, this queries the FAISS vector store\n",
    "    return {**state, \"documents\": []}\n",
    "\n",
    "\n",
    "def generate(state: RAGState) -> RAGState:\n",
    "    \"\"\"Node 3: Generate response based on retrieved documents and mode.\n",
    "\n",
    "    Generation strategy:\n",
    "    - Summary mode: Concise bullet-point summary\n",
    "    - Fact mode: Precise answer with specific details\n",
    "\n",
    "    Both modes use only the retrieved context (no external knowledge).\n",
    "    \"\"\"\n",
    "    # In real implementation, this invokes the LLM with appropriate prompt\n",
    "    return {**state, \"generation\": \"Generated response\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agentic RAG workflow graph compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "# Build the RAG workflow graph\n",
    "rag_graph = StateGraph(RAGState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "rag_graph.add_node(\"classify_mode\", classify_mode)\n",
    "rag_graph.add_node(\"retrieve\", retrieve)\n",
    "rag_graph.add_node(\"generate\", generate)\n",
    "\n",
    "# Define the workflow edges (linear flow)\n",
    "rag_graph.set_entry_point(\"classify_mode\")  # START â†’ classify_mode\n",
    "rag_graph.add_edge(\"classify_mode\", \"retrieve\")  # classify_mode â†’ retrieve\n",
    "rag_graph.add_edge(\"retrieve\", \"generate\")  # retrieve â†’ generate\n",
    "rag_graph.add_edge(\"generate\", END)  # generate â†’ END\n",
    "\n",
    "# Compile the graph\n",
    "rag_workflow = rag_graph.compile()\n",
    "\n",
    "print(\"âœ… Agentic RAG workflow graph compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the RAG Workflow\n",
    "\n",
    "The graph below shows the complete flow of the Agentic RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Generate and display the graph visualization\n",
    "    display(Image(rag_workflow.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(\"Note: Visualization requires graphviz. Error:\", e)\n",
    "    print(\"\\nGraph structure (text representation):\")\n",
    "    print(\"START â†’ classify_mode â†’ retrieve â†’ generate â†’ END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Analysis: Agentic RAG\n",
    "\n",
    "**Flow Description:**\n",
    "```\n",
    "__start__ â†’ classify_mode â†’ retrieve â†’ generate â†’ __end__\n",
    "```\n",
    "\n",
    "**Node Descriptions:**\n",
    "\n",
    "1. **START (`__start__`)**: Entry point for user queries\n",
    "\n",
    "2. **classify_mode**: \n",
    "   - **Type**: Agent Node (decision-making)\n",
    "   - **Purpose**: Analyzes query intent\n",
    "   - **Output**: Sets `mode` to \"summary\" or \"fact\"\n",
    "   - **Logic**: Keyword-based classification\n",
    "     - Summary triggers: \"summarize\", \"overview\", \"key points\"\n",
    "     - Fact triggers: \"when\", \"who\", \"where\", \"amount\", \"specific\"\n",
    "\n",
    "3. **retrieve**: \n",
    "   - **Type**: Agent Node (retrieval)\n",
    "   - **Purpose**: Fetches relevant documents from vector store\n",
    "   - **Output**: Populates `documents` list\n",
    "   - **Strategy**: \n",
    "     - Summary mode: 8 documents (broader context)\n",
    "     - Fact mode: 3 documents (focused retrieval)\n",
    "\n",
    "4. **generate**: \n",
    "   - **Type**: Agent Node (generation)\n",
    "   - **Purpose**: Creates LLM response using retrieved context\n",
    "   - **Output**: Populates `generation` field\n",
    "   - **Constraints**: Only uses retrieved documents (no external knowledge)\n",
    "\n",
    "5. **END (`__end__`)**: Terminal node, returns final state with `generation`\n",
    "\n",
    "**Key Features:**\n",
    "- **Linear workflow**: No conditional edges or loops\n",
    "- **Type-safe state**: TypedDict ensures proper data flow\n",
    "- **Mode-adaptive**: Different strategies for summaries vs. facts\n",
    "- **Grounded generation**: Responses strictly based on retrieved context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ReAct Agent (Search-Enabled Chat)\n",
    "\n",
    "The ReAct (Reasoning + Acting) agent is created using LangGraph's `create_react_agent` utility. This agent can:\n",
    "- Reason about user queries\n",
    "- Search the web using Tavily API\n",
    "- Combine search results with LLM knowledge\n",
    "\n",
    "### Architecture:\n",
    "The ReAct agent uses an internal graph structure (created by `create_react_agent`) with:\n",
    "- **Agent Node**: Decides whether to use tools or respond directly\n",
    "- **Tools Node**: Executes tool calls (e.g., Tavily web search)\n",
    "- **Conditional Edges**: Routes between agent and tools based on reasoning\n",
    "\n",
    "**Note**: The ReAct agent graph is pre-built by LangGraph. Below is a conceptual representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ReAct Agent Flow (Conceptual):\n",
      "================================\n",
      "\n",
      "       START\n",
      "         |\n",
      "         v\n",
      "    [Agent Node] <----------+\n",
      "         |                  |\n",
      "         |-- tool_call? ----|--- yes --> [Tools Node]\n",
      "         |                  |\n",
      "         |-- no ----------> END\n",
      "\n",
      "Node Descriptions:\n",
      "- Agent Node: LLM decides whether to use tools or respond\n",
      "- Tools Node: Executes Tavily web search\n",
      "- Conditional Edge: Routes based on agent's reasoning\n",
      "- Loop: Agent can make multiple tool calls for complex queries\n",
      "\n",
      "Key Features:\n",
      "âœ“ Streaming support for real-time responses\n",
      "âœ“ Multi-step reasoning (can chain multiple searches)\n",
      "âœ“ Timeout protection (default: 90 seconds)\n",
      "âœ“ Automatic tool integration (Tavily search)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ReAct Agent conceptual structure\n",
    "print(\"\"\"\n",
    "ReAct Agent Flow (Conceptual):\n",
    "================================\n",
    "\n",
    "       START\n",
    "         |\n",
    "         v\n",
    "    [Agent Node] <----------+\n",
    "         |                  |\n",
    "         |-- tool_call? ----|--- yes --> [Tools Node]\n",
    "         |                  |\n",
    "         |-- no ----------> END\n",
    "\n",
    "Node Descriptions:\n",
    "- Agent Node: LLM decides whether to use tools or respond\n",
    "- Tools Node: Executes Tavily web search\n",
    "- Conditional Edge: Routes based on agent's reasoning\n",
    "- Loop: Agent can make multiple tool calls for complex queries\n",
    "\n",
    "Key Features:\n",
    "âœ“ Streaming support for real-time responses\n",
    "âœ“ Multi-step reasoning (can chain multiple searches)\n",
    "âœ“ Timeout protection (default: 90 seconds)\n",
    "âœ“ Automatic tool integration (Tavily search)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Reference: ReAct Agent Setup\n",
    "\n",
    "The ReAct agent is created in `langchain_helpers.py` using the `AgentChatbotHelper.setup_agent()` method:\n",
    "\n",
    "```python\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Configure search tool\n",
    "tavily_search = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    "    tavily_api_key=tavily_api_key,\n",
    ")\n",
    "\n",
    "# Create agent with tools\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", streaming=True)\n",
    "agent = create_react_agent(llm, tools=[tavily_search])\n",
    "```\n",
    "\n",
    "**File Location**: `langchain_helpers.py:137-164`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Running the Agents\n",
    "\n",
    "### Example: Using the RAG Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Running the RAG workflow (requires actual setup)\n",
    "# This is for demonstration - actual implementation requires:\n",
    "# 1. Uploaded PDF files\n",
    "# 2. Built vector store\n",
    "# 3. Configured LLM\n",
    "\n",
    "example_query = {\n",
    "    \"question\": \"What are the key points in this document?\",\n",
    "    \"mode\": \"summary\",  # Will be set by classify_mode\n",
    "    \"documents\": [],\n",
    "    \"generation\": \"\",\n",
    "}\n",
    "\n",
    "print(\"Example RAG query:\")\n",
    "print(\"Question:\", example_query[\"question\"])\n",
    "print(\"\\nExpected flow:\")\n",
    "print(\"1. classify_mode â†’ Sets mode to 'summary' (keyword: 'key points')\")\n",
    "print(\"2. retrieve â†’ Fetches 8 documents from vector store\")\n",
    "print(\"3. generate â†’ Creates bullet-point summary from documents\")\n",
    "print(\"4. Returns grounded response\")\n",
    "\n",
    "# To actually run:\n",
    "# result = rag_workflow.invoke(example_query)\n",
    "# print(result[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using the ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Running the ReAct agent (requires API keys)\n",
    "# This is for demonstration - actual implementation requires:\n",
    "# 1. OpenAI API key\n",
    "# 2. Tavily API key\n",
    "\n",
    "example_search_query = \"What are the latest developments in AI for 2025?\"\n",
    "\n",
    "print(\"Example ReAct agent query:\")\n",
    "print(\"Question:\", example_search_query)\n",
    "print(\"\\nExpected flow:\")\n",
    "print(\"1. Agent analyzes query\")\n",
    "print(\"2. Decides to use Tavily search tool\")\n",
    "print(\"3. Executes web search\")\n",
    "print(\"4. Agent processes search results\")\n",
    "print(\"5. Synthesizes response combining search + LLM knowledge\")\n",
    "print(\"6. May loop for additional searches if needed\")\n",
    "\n",
    "# To actually run (async):\n",
    "# from langchain_helpers import AgentChatbotHelper\n",
    "# agent = AgentChatbotHelper.setup_agent(openai_key, tavily_key)\n",
    "# response = await AgentChatbotHelper.process_agent_response(agent, example_search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Comparison: RAG vs. ReAct Agent\n",
    "\n",
    "| Feature | Agentic RAG | ReAct Agent |\n",
    "|---------|-------------|-------------|\n",
    "| **Purpose** | Document QA | Web search + Chat |\n",
    "| **Data Source** | Uploaded PDFs (vector store) | Real-time web search (Tavily) |\n",
    "| **Graph Type** | Linear (fixed sequence) | Cyclic (tool-calling loops) |\n",
    "| **Nodes** | 3 agent nodes | 2 nodes (agent + tools) |\n",
    "| **Conditional Logic** | None (always follows same path) | Yes (decides tool usage) |\n",
    "| **Response Mode** | Mode-adaptive (summary/fact) | Single strategy |\n",
    "| **Context Grounding** | Strict (documents only) | Flexible (web + LLM) |\n",
    "| **Streaming** | Not implemented | Supported |\n",
    "| **Use Case** | Internal document analysis | Current events, research |\n",
    "\n",
    "### When to Use Each:\n",
    "\n",
    "**Agentic RAG**:\n",
    "- You have proprietary documents (PDFs, reports)\n",
    "- Need factually grounded responses\n",
    "- Require citation to source material\n",
    "- Want consistent, reproducible answers\n",
    "\n",
    "**ReAct Agent**:\n",
    "- Need current information (news, prices, etc.)\n",
    "- Want broad web knowledge\n",
    "- Require multi-step reasoning\n",
    "- Need to combine multiple sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Advanced: State Management\n",
    "\n",
    "Both agents use typed state management to ensure data consistency across nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG State - Strongly typed\n",
    "print(\"RAG State Structure:\")\n",
    "print(\"\"\"\n",
    "class RAGState(TypedDict):\n",
    "    question: str                      # User query (input)\n",
    "    mode: Literal[\"summary\", \"fact\"]  # Response strategy (set by classify_mode)\n",
    "    documents: List[Document]          # Retrieved context (set by retrieve)\n",
    "    generation: str                    # Final response (set by generate)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# ReAct State - Flexible message-based\n",
    "print(\"ReAct State Structure (simplified):\")\n",
    "print(\"\"\"\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"user query\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"...\", \"tool_calls\": [...]},\n",
    "        {\"role\": \"tool\", \"content\": \"search results\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"final response\"}\n",
    "    ]\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Integration in the Application\n",
    "\n",
    "### File References:\n",
    "\n",
    "1. **RAG Implementation**: `langchain_helpers.py:591-728`\n",
    "   - `RAGHelper.build_simple_agentic_rag()` - Builds the graph\n",
    "   - `RAGHelper.setup_rag_system()` - Complete setup with vector store\n",
    "\n",
    "2. **ReAct Implementation**: `langchain_helpers.py:129-228`\n",
    "   - `AgentChatbotHelper.setup_agent()` - Creates the agent\n",
    "   - `AgentChatbotHelper.process_agent_response()` - Async response handler\n",
    "\n",
    "3. **Streamlit Pages**:\n",
    "   - RAG Page: `pages/3_ðŸ“„_RAG_Document_Chat.py`\n",
    "   - Search Page: `pages/2_ðŸ”Ž_Search-Enabled_Chat.py`\n",
    "\n",
    "### Streamlit Integration Example:\n",
    "\n",
    "```python\n",
    "# From pages/3_ðŸ“„_RAG_Document_Chat.py\n",
    "from langchain_helpers import RAGHelper\n",
    "\n",
    "# Setup RAG system\n",
    "rag_workflow, pii_entities = RAGHelper.setup_rag_system(\n",
    "    uploaded_files,\n",
    "    api_key=openai_key,\n",
    "    anonymize_pii=True,  # Optional PII protection\n",
    "    use_cache=True       # Enable caching\n",
    ")\n",
    "\n",
    "# Process query\n",
    "result = rag_workflow.invoke({\"question\": user_query})\n",
    "answer = result[\"generation\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated the LangGraph agent architectures used in the LLM Bootcamp Project:\n",
    "\n",
    "### âœ… Agentic RAG Workflow\n",
    "- **Nodes**: START â†’ classify_mode â†’ retrieve â†’ generate â†’ END\n",
    "- **Type**: Linear workflow with no conditional edges\n",
    "- **Purpose**: Intelligent document QA with mode-adaptive retrieval\n",
    "\n",
    "### âœ… ReAct Agent\n",
    "- **Nodes**: Agent Node â†” Tools Node (with conditional looping)\n",
    "- **Type**: Cyclic workflow with tool-calling capability\n",
    "- **Purpose**: Web-enabled conversational AI\n",
    "\n",
    "### Key Takeaways:\n",
    "1. Both agents use **typed state management** for reliability\n",
    "2. RAG provides **grounded, factual** responses from documents\n",
    "3. ReAct enables **real-time web search** and multi-step reasoning\n",
    "4. Graphs are **compiled and optimized** by LangGraph for production use\n",
    "\n",
    "### Next Steps:\n",
    "- Run `streamlit run Home.py` to see the agents in action\n",
    "- Upload PDFs to test the RAG workflow\n",
    "- Try web search queries with the ReAct agent\n",
    "- Modify node logic in `langchain_helpers.py` to customize behavior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}