# version: '3.8'

services:
  streamlit-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-bootcamp-app
    ports:
      - "8502:8501"
    environment:
      # OpenAI API Key (configure via .env file)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      # Tavily API Key (configure via .env file)
      - TAVILY_API_KEY=${TAVILY_API_KEY:-}
      # MCP Server URL (configure via .env file)
      - MCP_SERVER_URL=${MCP_SERVER_URL:-}
      # Streamlit configuration
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - STREAMLIT_SERVER_HEADLESS=true
      - STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
    volumes:
      # Mount tmp directory for persistent uploads and cache
      - ./tmp:/app/tmp
      # Mount source code for development (optional - comment out for production)
      - .:/app
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "healthcheck.py"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Optional: MCP Server (FastMCP for prompt optimization)
  mcp-server:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-bootcamp-mcp-server
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    command: ["uv", "run", "python", "server.py"]
    restart: unless-stopped
    profiles:
      - mcp

volumes:
  tmp-data:
    driver: local

networks:
  default:
    name: llm-bootcamp-network
